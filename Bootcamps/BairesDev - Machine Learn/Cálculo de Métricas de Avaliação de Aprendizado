Entendendo a Matriz de Confusão

Antes de começarmos, vamos relembrar o que cada elemento da matriz de confusão representa:

VP (Verdadeiros Positivos): Casos em que o modelo previu corretamente a classe positiva.

VN (Verdadeiros Negativos): Casos em que o modelo previu corretamente a classe negativa.

FP (Falsos Positivos): Casos em que o modelo previu incorretamente a classe positiva (erro do tipo I).

FN (Falsos Negativos): Casos em que o modelo previu incorretamente a classe negativa (erro do tipo II).

Escolhendo uma Matriz de Confusão Arbitrária

Para fins de demonstração, vamos escolher uma matriz de confusão arbitrária. Isso nos permitirá calcular as métricas e entender como elas funcionam na prática. Vamos usar a seguinte matriz:

Predito Positivo	Predito Negativo
Real Positivo	VP = 80	FN = 20
Real Negativo	FP = 10	VN = 90
Cálculo das Métricas

Agora, vamos calcular cada uma das métricas usando as fórmulas fornecidas e os valores da nossa matriz de confusão:

Acurácia (Accuracy)

Fórmula: (VP + VN) / (VP + VN + FP + FN)

Cálculo: (80 + 90) / (80 + 90 + 10 + 20) = 170 / 200 = 0.85

Interpretação: A acurácia nos diz a proporção de previsões corretas em relação ao total de previsões. Neste caso, o modelo acertou 85% das previsões.

Sensibilidade (Recall)

Fórmula: VP / (VP + FN)

Cálculo: 80 / (80 + 20) = 80 / 100 = 0.80

Interpretação: A sensibilidade nos diz a proporção de casos positivos que foram corretamente identificados pelo modelo. Neste caso, o modelo identificou corretamente 80% dos casos positivos.

Especificidade (Specificity)

Fórmula: VN / (VN + FP)

Cálculo: 90 / (90 + 10) = 90 / 100 = 0.90

Interpretação: A especificidade nos diz a proporção de casos negativos que foram corretamente identificados pelo modelo. Neste caso, o modelo identificou corretamente 90% dos casos negativos.

Precisão (Precision)

Fórmula: VP / (VP + FP)

Cálculo: 80 / (80 + 10) = 80 / 90 = 0.8889 (aproximadamente)

Interpretação: A precisão nos diz a proporção de previsões positivas que foram realmente corretas. Neste caso, 88.89% das previsões positivas do modelo estavam corretas.

F-score (F1-score)

Fórmula: 2 * (Precisão * Sensibilidade) / (Precisão + Sensibilidade)

Cálculo: 2 * (0.8889 * 0.80) / (0.8889 + 0.80) = 2 * 0.71112 / 1.6889 = 1.42224 / 1.6889 = 0.8421 (aproximadamente)

Interpretação: O F-score é uma média harmônica entre precisão e sensibilidade. Ele busca um equilíbrio entre essas duas métricas. Neste caso, o F-score é de aproximadamente 0.84, indicando um bom equilíbrio entre precisão e sensibilidade.

Resumo das Métricas Calculadas

Acurácia: 0.85

Sensibilidade (Recall): 0.80

Especificidade: 0.90

Precisão: 0.8889

F-score: 0.8421

Considerações Finais

Contexto é Chave: A escolha da métrica mais adequada depende do problema em questão. Em alguns casos, a sensibilidade pode ser mais importante (detecção de doenças), enquanto em outros, a precisão pode ser mais relevante (filtragem de spam).

Desbalanceamento de Classes: Se as classes estiverem desbalanceadas (muitos exemplos de uma classe e poucos da outra), a acurácia pode ser enganosa. Nesses casos, é importante analisar outras métricas, como sensibilidade, especificidade e F-score.

Análise Conjunta: É fundamental analisar as métricas em conjunto para ter uma visão completa do desempenho do modelo.

Próximos Passos

Implementação em Código: Agora que entendemos os cálculos, podemos implementar essas métricas em Python usando bibliotecas como scikit-learn.

Análise de Diferentes Matrizes de Confusão: Experimente com diferentes matrizes de confusão para ver como as métricas variam.

Ajuste de Modelos: Use essas métricas para comparar diferentes modelos e ajustar seus parâmetros para obter o melhor desempenho.


import numpy as np
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

def calcular_metricas(matriz_confusao):
    """
    Calcula as métricas de avaliação de um modelo de classificação a partir de uma matriz de confusão.

    Args:
        matriz_confusao (np.ndarray): Matriz de confusão 2x2.

    Returns:
        dict: Dicionário contendo as métricas calculadas.
    """

    VP = matriz_confusao[0, 0]
    FN = matriz_confusao[0, 1]
    FP = matriz_confusao[1, 0]
    VN = matriz_confusao[1, 1]

    # Cálculo da acurácia
    acuracia = (VP + VN) / (VP + VN + FP + FN)

    # Cálculo da sensibilidade (recall)
    sensibilidade = VP / (VP + FN) if (VP + FN) > 0 else 0

    # Cálculo da especificidade
    especificidade = VN / (VN + FP) if (VN + FP) > 0 else 0

    # Cálculo da precisão
    precisao = VP / (VP + FP) if (VP + FP) > 0 else 0

    # Cálculo do F1-score
    f1 = 2 * (precisao * sensibilidade) / (precisao + sensibilidade) if (precisao + sensibilidade) > 0 else 0

    metricas = {
        "acuracia": acuracia,
        "sensibilidade": sensibilidade,
        "especificidade": especificidade,
        "precisao": precisao,
        "f1_score": f1
    }

    return metricas

# Exemplo de uso com a matriz de confusão que definimos anteriormente
matriz_confusao_exemplo = np.array([[80, 20], [10, 90]])

# Calculando as métricas
metricas_calculadas = calcular_metricas(matriz_confusao_exemplo)

# Imprimindo as métricas
print("Métricas calculadas a partir da matriz de confusão:")
for metrica, valor in metricas_calculadas.items():
    print(f"{metrica}: {valor:.4f}")

# Exemplo de uso com as funções do scikit-learn
y_true = np.array([1] * 100 + [0] * 100) # 100 positivos e 100 negativos
y_pred = np.array([1] * 80 + [0] * 20 + [1] * 10 + [0] * 90) # 80 VP, 20 FN, 10 FP, 90 VN

print("\n\nMétricas calculadas usando scikit-learn:")
print(f"Acurácia: {accuracy_score(y_true, y_pred):.4f}")
print(f"Sensibilidade (Recall): {recall_score(y_true, y_pred):.4f}")
print(f"Precisão: {precision_score(y_true, y_pred):.4f}")
print(f"F1-score: {f1_score(y_true, y_pred):.4f}")

# Matriz de confusão usando scikit-learn
print("\nMatriz de confusão usando scikit-learn:")
print(confusion_matrix(y_true, y_pred))



Explicação do Código

Importação das Bibliotecas:

numpy para manipulação de arrays (matriz de confusão).

sklearn.metrics para as funções de cálculo de métricas.

Função calcular_metricas:

Recebe uma matriz de confusão como entrada.

Extrai os valores de VP, FN, FP e VN da matriz.

Calcula as métricas (acurácia, sensibilidade, especificidade, precisão e F1-score) usando as fórmulas que discutimos.

Retorna um dicionário com as métricas calculadas.

Exemplo de Uso:

Cria uma matriz de confusão de exemplo (matriz_confusao_exemplo).

Chama a função calcular_metricas para obter as métricas.

Imprime as métricas calculadas.

Cria arrays y_true (valores reais) e y_pred (valores preditos) para usar as funções do scikit-learn.

Calcula e imprime as métricas usando as funções do scikit-learn.

Imprime a matriz de confusão usando o scikit-learn.

Resultados

O código irá imprimir as métricas calculadas usando a função que criamos e as funções do scikit-learn, além da matriz de confusão gerada pelo scikit-learn. Você verá que os resultados são consistentes com os cálculos que fizemos manualmente.
